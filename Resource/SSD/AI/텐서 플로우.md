## 텐서플로우 핵심 모듈
* ModelAPI
* LayersAPI
* Optimizer
* Losses

* 모델을 생성하는 Models 모듈
	* Sequential과 Functional API
		* Sequential : 한 갈래 텐서가 끝까지 흐르는 경우
		* Functional API : 텐서가 여러갈래로 나뉘는 경우
* 층을 쌓는 layers 모듈
	* 완전연결층 Dense, 컨볼루션층 Conv2D, 최대 풀링층 MaxPooling2D, ... • 손실함수를 위한 losses 모듈
* 평균제곱오차 MSE, 교차 엔트로피 categorial_crossentropy, 분할을 위한 focal, ...
* 옵티마지어를 위한 optimizers 모듈 § SDG, Adam, AdaGrad. RMSprop 등


## Pytorch 모델 생성
* torch.nn 모듈을 활용하여 Conv2d, MaxPool2d, Linear 함수를 사용
* 손실함수
	* 평균제곱오차 (MSE), 교차 엔트로피 (cross-entropy), categorical_cross- entropy, 분할을 위한 focal 등등
* 옵티마이저  
	* torch.optim 모듈을 사용 
	* SGD, Adam, AdaGrad 등

## 딥러닝의성공요인 
* 데이터셋의 확장
* 계산속도향상(GPU) 
* 학습 알고리즘의 향상

## 딥러닝알고리즘성능향상
* 손실함수  
* 옵티마이저
* 규제

## 손실 함수
* 신경망 초기에는 주로 평균제곱오차를 사용
* 딥러닝에서는 
	* 분류를 위해서 교차 엔트로피 주로 사용
	* 과업에 따른 다양한 손실함수가 제안됨
* Focal 손실 함수
	* 부류 불균형 문제에 효과적
	* 의료 영상 분할을 다룬 RetinaNet 논문이 제안
### 교차 엔트로피
* 참 값 벡터와 예측 벡터를 확률 분포로 간주
* 교차 엔트로피는 두 확률 분포의 다른 정로를 측정

## 옵티마이저
* 모멘텀적용 
	* 이전운동량이현재에영향을미치는물리법칙 
	* 가중치변경이력이현재가중치에영향을미침
* 적응적학습률적용
	* 
### 적응적 학습률

* 모멘텀의 표준 SGD는 고정된 학습률을 사용  
	* 작은 학습률 (0.001, 0.0001과 같은) 사용하여 보수적으로 이동
* 적응적 학습률에서는 상황에 따라 학습률을 조정함  
	* AdaGrad:이전그레디언트를누적한정보를이용해학습률을적응적으로결정 
	* RMSprop: 그레디언트를 누적할때 오래될수록 영향력을 감소하여 
	* AdaGrad 개선 
	* Adam: RMSprop에 모멘텀을 적용

## 규제

* 데이터 증강 (data augmentation)  
	* 훈련 집합을 조금씩 변형하여 인위적으로 늘림  
– 오프라인 방식과 온라인 방식 (주로 온라인 방식 사용)

§ 드랍 아웃 (Dropout)  
– 특징맵을구성하는요소중일부를랜덤선택하여0으로설정하여학습에배제 – 학습할때만적용하고예측과정에서는적용안함

§조기멈춤  
– 성능향상이없으면설정한세대수이전에학습을중지함