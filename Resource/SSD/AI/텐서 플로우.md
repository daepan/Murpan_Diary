## 텐서플로우 핵심 모듈
* ModelAPI
* LayersAPI
* Optimizer
* Losses

* 모델을 생성하는 Models 모듈
	* Sequential과 Functional API
		* Sequential : 한 갈래 텐서가 끝까지 흐르는 경우
		* Functional API : 텐서가 여러갈래로 나뉘는 경우
* 층을 쌓는 layers 모듈
	* 완전연결층 Dense, 컨볼루션층 Conv2D, 최대 풀링층 MaxPooling2D, ... • 손실함수를 위한 losses 모듈
* 평균제곱오차 MSE, 교차 엔트로피 categorial_crossentropy, 분할을 위한 focal, ...
* 옵티마지어를 위한 optimizers 모듈 § SDG, Adam, AdaGrad. RMSprop 등


## Pytorch 모델 생성
* torch.nn 모듈을 활용하여 Conv2d, MaxPool2d, Linear 함수를 사용
* 손실함수
	* 평균제곱오차 (MSE), 교차 엔트로피 (cross-entropy), categorical_cross- entropy, 분할을 위한 focal 등등
* 옵티마이저  
	* torch.optim 모듈을 사용 
	* SGD, Adam, AdaGrad 등

## 딥러닝의성공요인 
* 데이터셋의 확장
* 계산속도향상(GPU) 
* 학습 알고리즘의 향상

## 딥러닝알고리즘성능향상
* 손실함수  
* 옵티마이저
* 규제

## 손실 함수
* 신경망 초기에는 주로 평균제곱오차를 사용
* 딥러닝에서는 
	* 분류를 위해서 교차 엔트로피 주로 사용
	* 과업에 따른 다양한 손실함수가 제안됨
* Focal 손실 함수
	* 부류 불균형 문제에 효과적
	* 의료 영상 분할을 다룬 RetinaNet 논문이 제안
### 교차 엔트로피
* 참 값 벡터와 예측 벡터를 확률 분포로 간주
* 교차 엔트로피는 두 확률 분포의 다른 정로를 측정

## 옵티마이저
* 모멘텀적용 
	* 이전운동량이현재에영향을미치는물리법칙 
	* 가중치변경이력이현재가중치에영향을미침
* 적응적학습률적용
	* 
### 적응적 학습률

* 모멘텀의 표준 SGD는 고정된 학습률을 사용  
	* 작은 학습률 (0.001, 0.0001과 같은) 사용하여 보수적으로 이동
* 적응적 학습률에서는 상황에 따라 학습률을 조정함  
	* AdaGrad:이전그레디언트를누적한정보를이용해학습률을적응적으로결정 
	* RMSprop: 그레디언트를 누적할때 오래될수록 영향력을 감소하여 
	* AdaGrad 개선 
	* Adam: RMSprop에 모멘텀을 적용

## 규제

* 데이터 증강 (data augmentation)  
	* 훈련 집합을 조금씩 변형하여 인위적으로 늘림  
	* 오프라인 방식과 온라인 방식 (주로 온라인 방식 사용)
* 드랍 아웃 (Dropout)  
	* 특징맵을구성하는요소중일부를랜덤선택하여0으로설정하여학습에배제 
	* 학습할때만적용하고예측과정에서는적용안함
* 조기멈춤  
	* 성능향상이없으면설정한세대수이전에학습을중지함

## 전이 학습
* 어떤 도메인의 데이터로 학습한 모델을 다른 도메인에 적용하여 성능을 향상하는 방법


## 백본 모델
* 텐서플로가 제공하는 사전 학습 모델 
* 전이학습할때는보통이들모델을백본으로사용
![](https://i.imgur.com/bu2lkRc.png)

### 유명한사전학습모델
* VGGNet : 옥스포드 VGG 연구실에서 제작, 3x3의 컨볼루션 필터를 사용하 여층깊이를16으로깊게만듦
* GoogLeNet: 구글이 제작, 네트워크 속의 네트워크 아이디어 적용  
* ResNet : MS에서 제작, Residual connection을 활용하여 네트워크를 조금 더 깊게 쌓음

## VGG

* Deep Convolutional Network: VGGNet은 매우 깊은 네트워크로, VGG16은 16 개, VGG19는 19개의 레이어로 구성됩니다.
* Uniform Architecture: 네트워크 구조가 단순하며, 3x3 컨볼루션 필터와 2x2 맥스 풀링 레이어로 구성되어 있습니다.
* Receptive Field Size: 작은 3x3 필터를 여러 개 사용하는 방식으로, 동일한 크 기의 큰 필터보다 더 적은 파라미터로 복잡한 표현 학습이 가능합니다.
* Stacked Convolutions: 깊은 계층을 쌓아 올리는 방식으로, 네트워크가 더 깊 어질수록 더 복잡한 특징을 학습할 수 있게 설계되었습니다.

## GoogLeNet
* Inception Module: 다양한 크기의 컨볼루션 필터(1x1, 3x3, 5x5)와 맥스 풀링 (3x3)을 결합한 모듈을 사용하여, 여러 스케일에서 특징을 추출합니다.
* Network-in-Network: 1x1 컨볼루션을 사용하여 차원을 축소하고, 계산 비용 을 줄이는 동시에 비선형성을 도입해 모델의 표현력을 향상시킵니다.
* Deep Architecture with Efficiency: 22개 층으로 구성되어 있으면서도, 계산 효율성을 유지하도록 설계되었습니다.
* Auxiliary Classifiers: 학습 과정에서 "Gradient Vanishing" 문제를 해결하기 위 해 중간층에 두 개의 보조 분류기를 추가, 학습을 안정화하고 성능을 향상 시킵니다.

## ResNet
- Residual Learning: 깊은 네트워크에서 학습이 어려운 문제(Gradient Vanishing 문제 등)를 해결하기 위해 "Residual Block"을 도입하였습니다. 이 는 레이어 입력을 출력에 직접 더하는 "skip connection"을 사용합니다. 
- Deep Networks: ResNet은 50, 101, 152 등 매우 깊은 네트워크 구조(50, 101, 152-layer deep)를 제안하였으며, 이는 더 깊은 네트워크가 더 복잡한 특징 을 학습할 수 있음을 보여주었습니다. 
- Identity Mapping: Residual Block에서 추가된 skip connection은 신호가 그대로 전달되는 "identity mapping"을 제공합니다. 이를 통해 학습이 더 쉽고 빠 르게 이루어집니다. 
- Reduced Complexity: 깊은 네트워크 구조에도 불구하고, Residual Block을 통 해 모델 복잡성을 줄이고, 학습을 더 안정적으로 유지할 수 있습니다.


