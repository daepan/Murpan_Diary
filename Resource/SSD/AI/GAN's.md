생성적 적대 신경망 (Generative Adversarial Nets; NIPS 2014)

## 생성 모델이란?
생성 모델을 가장 단순하게 표현하면 p(x)

x가 발생할 확률 분포 p(x)를 알면 새로운 샘플을 생성할 수 있음
생성 모델은 레이블 없이 x만 가지고 확률 분포를 추정하므로 비지도 학습

Pdata = 데이터를 만들어 내는 진짜 확률 분포
Pmodel = 주어진 데이터셋으로부터 추정한 확률분포

## GAN란?
생성자generator와 판별자discriminator라는 적대자 간의 싸움
생성망 G와 분별망 D가 대립 관계 
생성망은 분별망을 속이려고 진짜 같은 가짜를 만들어냄 (위조 지폐범) 
분별망은 생성망이 만든 가짜와 훈련 집합에 있는 진짜를 정교하게 구별 (경찰)

### GAN - DCGAN
굿펠로우의 원래 논문은 완전연결층(Fully-connected layer)을 사용했는데 2016년 DCGAN은 CNN기반으로 변경
분별망 구조 – 28*28*1 영상(MNIST 가정)을 받아 0(가짜) 또는 1(진짜)을 출력 – 컨볼루션층은 LeakyReLU 사용 – 출력층의 활성 함수로 sigmoid 사용 § 생성망 구조 – 입력은 100 차원의 잠복 공간latent vector – 100 차원을 완전연결층으로 키운 다음 업 샘플링하는 컨볼루션층을 거쳐 28*28*1 영상을 생성
학습 § 일반적인 네트워크 보다 절차가 복잡함
분별망 학습 – 가짜와 진짜로 이진 분류하므로 쉬운 편 – 진짜는 훈련 집합, 가짜는 생성망에서 취해야 하므로 생성망과 연결된 상태에서 학습이 이루어져야 함. 이전 페이지 그림은 이런 관계를 설명 § 생성망 학습은 더 복잡 – 난수로 100 차원의 잠복 공간의 벡터 생성하고 생성망을 통과시켜 가짜 샘플을 생성 – 생성된 가짜 샘플에 부류 1(진짜)을 부여하고 학습 진행. 이때 분별망의 가중치 를 고정해 생성망만 학습이 일어나게 설정 – 현재 분별망이 가짜를 진짜로 오인하도록 생성망을 학습함으로써 생성망이 더욱 진짜 같은 샘플을 만들도록 개선하는 전략 – 분별망 가중치도 학습이 되게 허용하면 생성망이 개선되는 대신 분별망이 퇴화할 수도 있음

• GAN의 모드 붕괴mode collapse 현상 § 학습률 조정 – 생성망과 분별망의 학습률을 적절히 조절하여 네트워크가 균형있게 학습하도 록 함 § 다양한 정규화 방법 – Batch Norm과 같은 정규화 기법을 이용하여 학습의 안정성을 높이고, 특정 모드 에 과도하게 수렴하는 것을 방지 § Alternative Loss function – Wasserstein GAN (WGAN)과 같은 대안적 손실 함수 제안


### GAN – WGAN
Wasserstein GAN (WGAN) § 2017년 아르좁스키의 논문에서 제안됨 – WGAN은 GAN을 안정적으로 학습하기 위해 제안됨 § 다음 두 가지 속성이 있는 GAN을 학습하기 위한 방법 – 생성자가 수렴하는 것과 샘플의 품질을 연관 짓는 의미 있는 손실 측정 방법 – 최적화 과정의 안정성 향상 § Wasserstein Distance를 이용

Wasserstein Distance는 결합 분포 �(�, �)를 통해 � → �로 변환하는 작업량 을 최소화함
Wasserstein Distance를 사용해야 하는 이유 § 기존의 GAN (DCGAN)은 JS-Divergence를 최소화하는 손실 함수로 학습 – p, q 즉 실제 데이터 분포와 생성 데이터 분포가 겹치지 않을 때 상수값을 가짐.
GAN은 불안정한 학습을 보이는 반면, WGAN은 안정적인 학습을 보임

WGAN 손실 함수 § Wasserstein loss function은 [0, 1] 대신 [-1, 1]을 사용 – 따라서, DCGAN의 Discriminator에서 사용한 마지막 sigmoid를 삭제 (Sigmoid는 결과 값을 (0, 1) 사이로 만들기 때문) – WGAN은 Discriminator (Critic)에 활성함수를 사용하지 않아, [−∞, ∞]범위의 숫자 가 될 수 있도록 만듦
## 다룸 가능(tractable)
에서 무수히 많이 샘플링하여 데이터셋 를 무한히 크게 하면 로 추정한 은 와 같게 됨


### 다룸 가능한 가우시안을 이용한 생성 모델
* 가우시안을 k개 혼합하여 실제 분포에 더 가깝게 모델링

데이터 X를 위한 최적 매개변수 값은 보통 EM(Expectation Maximization) 알고리즘으로 알아냄

#### 가우시안 혼합 모델의 한계 
* 가우시안 혼합 모델은 데이터가 가우시안 분포를 한다는 가정을 통해 다룸 가능 확보 
* 실제로는 데이터가 가우시안 분포와 멀기 때문에 생성 모델로서 한계를 보임



## 다룸 불가능
