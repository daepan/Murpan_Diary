## 1. CNN's
### Convolutional Neural Networks

ML 한계
* MLP의한계
	* 2차원 구조의 영상을 1차원으로 변환하여 입력. 적지 않은 정보 손실로 인한 성능저하
	* 인간은 2차원에서 특징 추출  
	* 수용장(receptive field)이라는 작은 영역에서 특징을 추출

## CNN(Convolutional Neural Networks)
* 컨볼루션 신경망은 인간 시각을 모방
* 딥러닝 중 가장 성공적인 예시
* CNN은 컨볼루션 연산이 핵심
* 일반적인 경우엔 사람이 필터설계 - 가우시안 스무딩, 소벨 에지, 케니 에지
	* 이 필터는 어디서 왔는지?
	* 인식에 최적인가?
	* 데이터셋에 맞는 최적 필터를 사용해야하는지?
* CNN의 핵심 아이디어는 **"최적의 필터를 학습"** 으로 알아냄

### 컨볼루션 층과 풀링 층
* 컨볼루션 층은 표준 컨볼루션에서 몇가지 확장이 필요하다
	* 필터를 여러개 사용해서 다양한 데이터를 추출한다.
	* 하나의 필터는 바이어스 하나를 갖게 된다.
* 컨볼루션 층의 연산
	* 필터 n개 당 출력 특징 맵의 개수가 n개가 나온다.
	![](https://i.imgur.com/OnNbNxA.png)
![](https://i.imgur.com/EX8iiv4.png)

* **가중치 공유weight sharing와 부분 연결성partial connection**
* 입력 특징 맵의 모든 화소가 같은 필터 사용하니 가중치를 공유하는 셈
* 필터는 해당 화소 주위에 국한하여 연산 수행. 가중치 개수가 획기적으로 줄어듦
	* k’개의 h * h * k 필터를 쓰는 경우 가중치는 k’(kh2+1)개

### 컨볼루션층의 연산량
![](https://i.imgur.com/51vnzMp.png)

첫번째층  
– 입력은 256 * 256 * 3 텐서. 0 덧대기하고 보폭 2이고 3 * 3 필터를 64개 사용하므로

출력은 128*  128 * 64 텐서 (필터 모양은 3 * 3 *. 3). – 128 * 128 * 28 * 64번의곱셈수행

두번째층  
– 입력은 128 * 128 *. 64 텐서. 0 덧대기하고. 보폭. 2이고. 5 *. 5 필터를 128개 사용하므

로 출력은 64 * 64 * 128 텐서 (필터 모양은 5 * 5 * 64) – 64 * 64 * 1601 * 128번의곱셈수행

### 풀링층
* 고정된 알고리즘으로 값을 취한다.
* 최대 풀링은 필터 안의 화소의 최대값 취함
* 평균 풀링은 필터 안의 화소의 평균을 취함
* 지나친 상세함을 줄이는 효과와 특징 맵의 크기를 줄이는 효과

### 다층 퍼셉트론
* 영상을 입력하기 위해 1차원으로 펼칠때 정보 손실
* FC층의 과다한 매개변수
	* 256 * 256 * 3영상을 입력하려면 196608개의 입력노드 필요

### 인간의 수용장을 모방하는 컨볼루션 신경망 등장
* 1980년 = 네오코그니트론
* 1998년 르쿤의 논문 : LeNet-5를 구현하여 수표 자동 입력 시스템 구현
* 2010년 ImageNet 데이터셋 탄생 ILSVRC 대회 개최 시작
* 2012년 AlexNet이 15.3%오류율로 우승 이후 CNN 관심이 커짐
* 이후 VGG, GoogLeNet, ResNet이 우승
* 물체 검출 분할 추적 자세 추정등의 다양한 모델 등장
* 2014년 생성모델 GAN 등장
* 알고리즘의 발전
	* ReLu 등 활성함수
	* 교차 엔트로피 등 손실 함수
	* 드롭 아웃등 규제 기법
	* Adam 등 옵티마이저 

### 딥러닝의 성공요인
* 인터넷으로 인해 커진 데이터셋
* GPU로 인해 값싸게 병렬처리 가능해짐
* 학습 알고리즘의 발전

### 딥러닝 알고리즘 성능 향상
- 손실함수
- 옵티마이저
- 규제


### 빌딩 블록을 쌓아 만드는 컨볼루션 신경망
* 빌딩 블록 쌓기
	* 보통 컨볼루션층과 풀링층을 번갈아 쌓는다.
	* 풀링층에서는 텐서 깊이가 유지된다.
	* 신경망의 앞부분은 특징 추출, 뒷부분은 분류를 담당한다.
* LeNet 사례가 존재
* 유연한 구조
	* 문제에 따라 다양한 모양으로 조립 가능
	* 오토 인코더: 입력과 출력이 같은 신경망, 비지도 학습
	* 영상분할을 위한 컨볼루션 신경망

### 컨볼루션 신경망의 학습
* 역전파 학습 알고리즘 사용
	* 컨볼루션층의 커널 화소와 온전연결층의 에지가 가중치에 대당 
	* 풀링층은 가중치 없음
* 특징 학습
	* 학습 알고리즘은 주어진 데이터 셋을 인식하는데 최적인 필터를 알아낸다.
### 텐서 플로 프로그래밍
- 모델을 생성하는 models 모듈
	- Sequential은한갈래텐서가끝까지흐르는경우 
	- Functional API는 텐서가 여러 갈래로 나뉘는 경우
- 층을 쌓는 layers 모듈
	- 완전연결층 Dense, 컨볼루션층 Conv2D, 최대 풀링층 MaxPooling2D, ... 
- 손실함수를 위한 losses 모듈
	- 평균제곱오차 MSE, 교차 엔트로피 categorial_crossentropy, 분할을 위한 focal, ...
- 옵티마지어를 위한 optimizers 모듈 
	- SDG, Adam, AdaGrad. RMSprop 등

### 손실함수
- 교차 엔트로피
	- 주로 분류 문제에서 사용되는 손실 함수로, 모델이 예측한 확률 분포와 실제 정답 분포 사이의 차이를 측정
	- 분류 문제에서 **정확한 클래스 확률 예측**을 유도해 모델이 더욱 확신에 찬 예측을 할 수 있게 합니다.
	- 평균제곱오차와의 차이
		- 평균제곱오차는 예측 값과 실제 값 간의 차이를 제곱하여 평균한 값입니다. 제곱이 적용되므로 큰 오차에 대해 패널티가 더 커지며, 예측 값이 실제 값과 정확히 일치할수록 손실이 낮아집니다.
		- 분류 모델에서의 분류 문제에서 모델의 예측 정확도를 더 높이기 위해서이며, 평균 제곱 오차보다 더 모델이 예측하는 확률분포의 차이를 더 크게 반영하기에 더 많이 사용된다.
- Focal 손실 함수
	- 불균형데이터를 더 효과적으로 학습하기 위한 손실함수
	- 교차 엔트로피 손실 함수에 **가중치를 부여**하여, 모델이 잘 예측하지 못한 **어려운 샘플에 더 높은 손실을 할당**하도록 설계되었으며, 물체 탐색에 주로 이용된다.
- 옵티마이저
	- 기본 옵티마이저에 SGD를 개선하는 전략
	- 모멘텀 적용
		- 이전 운동량이 현재에 영향을 미치는 물리 법칙이며, 가중치 변경 이력이 현재 가중치에 영향을 준다
	- 적응적 학습률 적용
		- 모멘텀의 SGD는 고정된 학습률을 사용하지만, 이는 상황에 따라 학습률을 조절한다.
### 규제
- 다양한 규제 기법
	- 데이터 증강
		- 훈련 집합을 조금씩 변형하여 인위적으로 늘린다.
	- 드랍 아웃
		- 특징 맵을 구성하는 요소 중 일부를 내덤 선택하여 0으로 설정하여 학습에 배제
		- 학습할 떄만 적용하며 예측 과정에서는 적용하지 않는다.
	- 조기 멈춤
		- 성능 향상이 없는 경우 설정한 세대수 이전에 학습을 중지
### 전이 학습
- 어떤 도메인의 데이터로 학습한 모델을 다른 도메인에 적용하여 성능을 향상하는 방법

### 백본 모델
- 유명한 사전 학습 모델
	- VGGNet
		- **깊은 컨볼루션 네트워크**: VGG16은 16개, VGG19는 19개의 레이어로 구성된 깊은 신경망입니다.
		- **단순한 구조**: 모든 레이어가 3x3 컨볼루션 필터와 2x2 맥스 풀링 레이어로 이루어져 있으며, 네트워크 구조가 규칙적입니다.
		- **작은 필터 사용**: 3x3 필터를 여러 개 쌓아 큰 필터를 대체하면서 적은 파라미터로 복잡한 표현 학습이 가능합니다.
		- **계층적 특징 학습**: 깊은 레이어를 통해 더 복잡한 특징을 점진적으로 학습합니다.
	- GoogLeNet
		- **Inception Module**: 1x1, 3x3, 5x5 컨볼루션 필터와 3x3 맥스 풀링을 결합하여 다양한 스케일에서 특징을 추출합니다.
		- **Network-in-Network**: 1x1 컨볼루션으로 차원을 축소하여 계산 비용을 줄이면서 비선형성을 추가해 표현력을 높였습니다.
		- **효율적인 깊은 아키텍처**: 22개 레이어로 구성되어 있으며, 계산 효율성을 유지하도록 설계되었습니다.
		- **보조 분류기 (Auxiliary Classifiers)**: 중간층에 보조 분류기를 추가하여 Gradient Vanishing 문제를 해결하고 학습을 안정화합니다.
	- ResNet
		- **Residual Learning**: Residual Block을 도입하여 Gradient Vanishing 문제를 해결하고, 학습을 용이하게 합니다.
		- **매우 깊은 네트워크**: 50, 101, 152 레이어 등 깊은 네트워크 구조를 통해 복잡한 특징을 학습할 수 있습니다.
		- **Identity Mapping**: Residual Block에 skip connection을 추가하여 신호를 그대로 전달하며, 학습 속도를 높입니다.
		- **복잡성 감소**: Residual Block을 통해 깊은 구조에서도 모델의 복잡성을 줄이고 안정적인 학습을 유지할 수 있습니다.

## 영상 인식
### 인식
- 인식
	- 사람과 달리 컴퓨터 문제는 세부 문제로 구분
	- 별도의 데이터셋과 성능 기준을 가지고 알고리즘을 개발
- 분류
	- 영상에 있는 물체으 부류를 알아내는 부류 확률 벡터를 출력
	- 특정 물체 감지
	- 고양이나 자전거 같은 일반적인 물체를 알아내는 범주
- 검출
	- 영상에서 물체를 찾아 직사각형으로 위치 표현
- 추적
	- 비디오에 나타난 물체의 이동궤적을 표시
- 행동 분류
	- 물체가 수행하는 행동의 종류를 알아내는 문제

### 데이터셋과 방법론의 공진화
- DeepFashion
	- 50부류에 대한 80만장 영상
- Food-101
	- 101부류에 대한 약 10만장 영상
- CheXpert
	- 65240환자의 엑스레이 224315장
- DIV2K  
	- Super-resolution을 위한 고해상도 이미지 (2K 이미지) 1,000장 영상
-  GoPro Dataset  
	- 모션디블러링을위한고속이동촬영동영상
- Flickr2K  
	- Super-resolution을 위한 고해상도 이미지 (2K 이미지) 2,000장 영상

- 합리적이고 공정한 성능 평가 척도가 중요
	- 세부 문제 에 따른 다양한 척도

- 영상 레이블링
	- 많은 노동력이 필요하며 검출 레이블링은 분류 레이블링 보다 어렵고 분할 레이블링은 검출 레이블링보다 어렵다
- 사람과 컴퓨터 비전은 인식하는 방식이 매우 다르다.
	- 사람은 뭐 세밀하고 즉각적이고 추론도 하고 별걸 다한다.
	- 컴퓨터 비전은 세부 문제별로 독립적으로 해결한다. 위에서 말했던 것 처럼 분류
		- 능동적이지 않으며, 사람의 의도를 고안하여 학습하는 것은 먼 미래이다.

### 분류
- 사례 분류
	- 모양과 텍스처가 고정된 특정 물체를 분류
	- 물체를 구성하는 직선과 곡선을 분석하는 기하학적인 방법이 주류
- 범주 분류
	- 코끼리나 자전거 같은 일반 부류의 물체를 분류
	- 부류 내 변화가 아주 심하며, 혼재와 가림을 허용하여 더욱 어려움
	- 고전적으로 부품기반 방법을 사용
- 주로 범주 분류에 대해 많은 연구가 진행되고 있다.
- 미세 분류 문제
	- 부류 내 변화가 크고, 부류간 변화가 적어 아주 어려운 문제
- 설명 가능
	- 인간은 의사결정의 이유를 설명하는 능력이 뛰어나며, 분류 결과에 대한 이유를 설명하지 못하는 치명적 한계가 존재한다.
- 적대적 공격과 방어 전략
	- 딥러닝 모델을 속이려는 시도가 쉽게 통하며, 잡음이 섞이면 타조라고 분류하기도 한다.

### 검출
- 검출 문제
	- 물체의 위치와 함께 부류 정보를 알아내야하므로 분류보다 어렵다
	- 검출 알고리즘은 물체의 위치와 부류 신뢰도를 나타내는 확률벡터를 출력
	- 딥러닝으로 전환하면서 획기적 발전이 이루어짐
	- 성능 척도로는  mAp를 주로 사용한다.
- AP 알고리즘
	- 객체 검출 모델의 성능을 평가하는 방법
	- 주로 객체 검출에서 정밀도(Precision)와 재현율(Recall)을 기준으로 AP 값을 구한다.
	-  **정밀도(Precision)** 가 높을수록: 모델이 예측한 객체 중 실제로 올바르게 탐지된 비율이 높다는 것을 의미합니다. 즉, **잘못된 탐지가 적다는 뜻**으로, 모델이 **오탐(false positive)을 줄이고 정확한 예측을 하고 있다**고 볼 수 있습니다.
	- **재현율(Recall)** 이 높을수록: 실제 객체 중 모델이 탐지한 비율이 높아진다는 의미입니다. 즉, **누락된 탐지가 적다는 뜻**으로, 모델이 **실제 객체를 빠뜨리지 않고 잘 탐지**하고 있음을 나타냅니다.
- 검출 : 고전 방법
	- HOG 특징 추출과정  타일을 생성해서 각 정규화를 통해 확인
	- 후보영역의 사람 여부는 SVM 분류기 사용
	- 후보 영역은 여러 해상도 영상에 슬라이딩 윈도우를 적용하여 생성
	- 이후 DPM 등장으로 물체를 몇개의 부품으로 모델링
- 검출
	- 딥러닝으로 전환되어 RCNN 방법이 되었다.
	- RCNN을 대표하는 두 단계 방법
		- 후보 영역을 생성하고 분류를 통해 물체 여부 판정
		- 영역 제안 단계
			- 물체가 있을 가능성이 높은 영역을 찾는 단계
			- 선택적 알고리즘을 주로 채택
		- 영역 분류 단계
			- 227 * 227로  정규화하여 컨볼루션 신경망을 4096차원 특징 추출
		- 한계
			- 선택적 탐색은 한 장 영상 처리하는데 CPU에서 2초 가량 소요
			- 영역 제안 단계가 병목
	- YOLO가 대표하는 한 단계 방법
		- 위치와 부류 정보를 묶어 참값을 만들고 신경망이 참값을 알아내도록 유도한다.
			- RCNN 보다 정확률이 떨어지지만 월등히 빠르다
		- 영상을 s * s 분할
		- 물체 중심에서 해당한 칸이 물체를 책임지게 된다.
		- 학습 방식
			- 90차원 벡터는 박스 정보와 원핫 코드의 부류 정보가 섞여있다.
			- 5개의 항으로 구성된 손실 함수 사용을 통해 통째 학습을 진행

### 분할
- 딥러닝 기반 분할
	- 셀수 있는 물체, 셀 수 없는 물체
	- 의미 분할
		- 모든 화소에 부류할당, 같은 부류는 구분하지 않는다.
	- 사례 분할
		- 셀수 있는 물체만 분할, 같은 부류 물체 여럿이면 번호를 붙여 구별
	- 총괄 분할
		- 모든 화소에 부류 할당, 같은 부류 물체가 여럿이면 번호 붙여 구분
- 분할 : 척도
	- 분할을 밀집 분류 문제로 간주하고 분류 척도 사용
	- IoU에 기반한 척도: AP와 mAP, Dice 계수 를 활용
- 분할  : 데이터 셋
	- 분할 레이블을 제공하거나 특수 목적 또는 데이터셋이 존재한다.
	- FCN
		- 분할을 처음 시도한 성공적인 컨볼루션 신경망
		- 다운 샘플링으로 맵 크기를 줄인 다음 업 샘플링으로 원래 크기 복원
- 전치 컨볼루션
	- 맵의 크기를 복원하는 것이지, 값을 복원하는 것은 아니다.
	- 필터를 학습으로 알아내기 떄문에 값을 복원할 필요가 없다.
	- FCN 이후 다양한 CNN 기반 신경망 등장
		- DeconvNet
		- U-Net
		- DeepLab v3+
### 사람 인식
- 가장 많은 관심과 연구 진행
	- 얼굴 인식은 사람 인식 중에 가장 많이 연구됨
		- 고전 방법 고유 얼굴 기법
			- 얼굴 영상에 주성분 분석을 적용하여 차원을 줄인 다음 매칭 알고리즘으로 인식
			- 정면 얼굴의 한계가 있음
		- 컨볼루션 신경망 시대
			- DeepFace 사람 수준에 근접
	- 얼굴 확인
		- 두 장의 얼굴이 동일인 인지 확인
	- 얼굴 인식
		- 미세 분류의 일종
		- 같은 부류 영상은 유사도 높게 유지하고 다른 부류는 낮게 유지하는 손실함수
## 3. GAN's
생성적 적대 신경망 (Generative Adversarial Nets; NIPS 2014)
## 생성 모델이란?
- 생성 모델을 가장 단순하게 표현하면 p(x)

- x가 발생할 확률 분포 p(x)를 알면 새로운 샘플을 생성할 수 있음
- 생성 모델은 레이블 없이 x만 가지고 확률 분포를 추정하므로 비지도 학습

- Pdata = 데이터를 만들어 내는 진짜 확률 분포
- Pmodel = 주어진 데이터셋으로부터 추정한 확률분포

## GAN란?
- 생성자generator와 판별자discriminator라는 적대자 간의 싸움
- 생성망 G와 분별망 D가 대립 관계 
- 생성망은 분별망을 속이려고 진짜 같은 가짜를 만들어냄 (위조 지폐범) 
- 분별망은 생성망이 만든 가짜와 훈련 집합에 있는 진짜를 정교하게 구별 (경찰)

### GAN - DCGAN
- CNN 기반 생성형 네트워크
- 고해상도 이미지를 더 정교하게 생성하며, 비교적 안정적으로 학습이 가능하다
- 여전히 훈련 과정이 불안정해, 학습 초기에 손실 진폭이 크게 변하거나 균형을 맞추기 어려울 수 있습니다.
- 분별망에서는 교차엔트로피 , 생성망에서는 평균오차를 손실함수로 채택


### GAN – WGAN
- 기존 GAN의 훈련 불안정 문제를 해결하기 위해 제안된 모델로, **Wasserstein 거리**(지구 거리라고도 함)를 사용해 **두 분포 간 차이**를 측정
- Wasserstein Distance를 사용하는 이유
	- 기존의 GAN은 JS-Divergence를 최소화하는 손실 함수로 학습
	- WGAN은 안정적인 학습을 보인다.
- Wasserstein distance를 안정적으로 계산하기 위해선 discriminator가 1-립 시츠여야한다.
	- 왜 만족해야하는가?
		- 분포 간 거리 차이를 정확히 평가
		- discriminator의 기울기가 1보다 커지게 된다면, 생성망이 학습 도중 큰 기울기를 받아 들이기 되기 때문에 불안정해짐
- WGAN-GP
	- 가중치 클리핑 대신 Gradient penalty를 사용하여 단점을 해결한 방식
### DCGAN과 WGAN의 차이점

1. **손실 함수**:
    - **DCGAN**: 전통적인 GAN 손실 함수(이진 교차 엔트로피) 사용.
    - **WGAN**: Wasserstein 거리 기반 손실 함수 사용으로, 학습이 안정적이고 손실 값의 의미가 더 명확.
2. **훈련 안정성**:
    - **DCGAN**: CNN을 적용해 일반 GAN보다 안정적이지만, 여전히 불안정한 학습 가능성이 존재.
    - **WGAN**: Wasserstein 거리로 인해 안정적인 학습 가능, 손실 진폭이 안정적으로 유지되어 학습 진행이 명확하게 관찰됨.
3. **모델 목적**:
    - **DCGAN**: 고해상도 이미지 생성과 세밀한 이미지 품질에 집중.
    - **WGAN**: GAN의 훈련 안정성과 수렴 문제 개선에 중점.

요약하자면, **DCGAN은 이미지 품질을 개선**하는 데 주력한 반면, **WGAN은 GAN 훈련의 불안정성을 해결**하는 데 초점을 맞추어 각각 GAN의 성능과 학습 안정성을 높인 모델입니다.

### 다룸 가능과 다룸 불가능
- 주로 문제 해결 또는 처리 가능성을 평가할 때 사용하는 개념으로, 시스템이나 사람, 프로세스가 **어떤 상황을 처리하거나 조정할 수 있는지 여부**를 나타낸다.
- 다룸 불가능은 표현도 학습도 샘플링도 불가능하며, 근사하는 방법 뿐이다.

### 다룸 가능한 가우시안을 이용한 생성 모델
* 가우시안을 k개 혼합하여 실제 분포에 더 가깝게 모델링
- 데이터 X를 위한 최적 매개변수 값은 보통 EM(Expectation Maximization) 알고리즘으로 알아냄

#### 가우시안 혼합 모델의 한계 
* 가우시안 혼합 모델은 데이터가 가우시안 분포를 한다는 가정을 통해 다룸 가능 확보 
* 실제로는 데이터가 가우시안 분포와 멀기 때문에 생성 모델로서 한계를 보임



## 4. Auto Encoder
![](https://i.imgur.com/9ZNAKn8.png)

### 오토 인코더란?
- 입력을 받아 효율적인 내부표현으로 바꾸고, 입력과 가장 가까운 어떤 것을 출력함
	- 인코더, 잠재공간, 디코더로 이루어져 있다.

### 변형 인코더 
- DAE 디노이징 오토 인코더
	- 입력 데이터에 노이즈를 추가해서 복원을 어렵게 해서 복원을 더 잘하도록 한다.
	- 일반적인 오토 인코더와의 차이점
		- 입력 데이터에 가우시안 노이즈 추가
		- 입력으로는 노이즈가 추가된 데이터, 레이블로는 원본 데이터
		- 잠재공간에서 좀 더 가치 있는 표현의 추출이 가능함
- SAE 희소 오토 인코더
	- 잠복공간에 뉴런 수를 감소시키는 기법
		- 여기서 뉴런을 감소시킨다는 것은 인코딩 과정에서 잠복공간에 도달하는 각 뉴런들의 수를 감소시킨다는 것이다.
		- 이 잠복공간 내에 있는 뉴런이 감소함으로 디코딩을 위해서는 최적의 뉴런을 추출해야하기에 기존 오토인코더 보다 인코딩에서 더 가치 있는 표현을 추출하게 된다.
	- 희소를 구현하는 간단한 방법  
		- 잠재공간을 0~1사이값으로 제한하기 위해 시그모이드 활성화 함수 사용 
		- 잠재공간층의 활성화 값에 L1 규제 추가
		- 희소를 강제할 수 있는 로스 함수 추가

## 5. 변형 오토인코더
