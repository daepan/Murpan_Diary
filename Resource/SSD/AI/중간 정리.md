## 1. CNN's
### Convolutional Neural Networks

ML 한계
* MLP의한계
	* 2차원 구조의 영상을 1차원으로 변환하여 입력. 적지 않은 정보 손실로 인한 성능저하
	* 인간은 2차원에서 특징 추출  
	* 수용장(receptive field)이라는 작은 영역에서 특징을 추출

## CNN(Convolutional Neural Networks)
* 컨볼루션 신경망은 인간 시각을 모방
* 딥러닝 중 가장 성공적인 예시
* CNN은 컨볼루션 연산이 핵심
* 일반적인 경우엔 사람이 필터설계 - 가우시안 스무딩, 소벨 에지, 케니 에지
	* 이 필터는 어디서 왔는지?
	* 인식에 최적인가?
	* 데이터셋에 맞는 최적 필터를 사용해야하는지?
* CNN의 핵심 아이디어는 **"최적의 필터를 학습"** 으로 알아냄

### 컨볼루션 층과 풀링 층
* 컨볼루션 층은 표준 컨볼루션에서 몇가지 확장이 필요하다
	* 필터를 여러개 사용해서 다양한 데이터를 추출한다.
	* 하나의 필터는 바이어스 하나를 갖게 된다.
* 컨볼루션 층의 연산
	* 필터 n개 당 출력 특징 맵의 개수가 n개가 나온다.
	![](https://i.imgur.com/OnNbNxA.png)
![](https://i.imgur.com/EX8iiv4.png)

* **가중치 공유weight sharing와 부분 연결성partial connection**
* 입력 특징 맵의 모든 화소가 같은 필터 사용하니 가중치를 공유하는 셈
* 필터는 해당 화소 주위에 국한하여 연산 수행. 가중치 개수가 획기적으로 줄어듦
	* k’개의 h * h * k 필터를 쓰는 경우 가중치는 k’(kh2+1)개

### 컨볼루션층의 연산량
![](https://i.imgur.com/51vnzMp.png)

첫번째층  
– 입력은 256 * 256 * 3 텐서. 0 덧대기하고 보폭 2이고 3 * 3 필터를 64개 사용하므로

출력은 128*  128 * 64 텐서 (필터 모양은 3 * 3 *. 3). – 128 * 128 * 28 * 64번의곱셈수행

두번째층  
– 입력은 128 * 128 *. 64 텐서. 0 덧대기하고. 보폭. 2이고. 5 *. 5 필터를 128개 사용하므

로 출력은 64 * 64 * 128 텐서 (필터 모양은 5 * 5 * 64) – 64 * 64 * 1601 * 128번의곱셈수행

### 풀링층
* 최대 풀링은 필터 안의 화소의 최대값 취함
* 평균 풀링은 필터 안의 화소의 평균을 취함
* 지나친 상세함을 줄이는 효과와 특징 맵의 크기를 줄이는 효과

### 다층 퍼셉트론
* 영상을 입력하기 위해 1차원으로 펼칠때 정보 손실
* FC층의 과다한 매개변수
	* 256 * 256 * 3영상을 입력하려면 196608개의 입력노드 필요

### 인간의 수용장을 모방하는 컨볼루션 신경망 등장
* 1980년 = 네오코그니트론
* 1998년 르쿤의 논문 : LeNet-5를 구현하여 수표 자동 입력 시스템 구현
* 2010년 ImageNet 데이터셋 탄생 ILSVRC 대회 개최 시작
* 2012년 AlexNet이 15.3%오류율로 우승 이후 CNN 관심이 커짐
* 이후 VGG, GoogLeNet, ResNet이 우승
* 물체 검출 분할 추적 자세 추정등의 다양한 모델 등장
* 2014년 생성모델 GAN 등장
* 알고리즘의 발전
	* ReLu 등 활성함수
	* 교차 엔트로피 등 손실 함수
	* 드롭 아웃등 규제 기법
	* Adam 등 옵티마이저 

### 딥러닝의 성공요인
* 인터넷으로 인해 커진 데이터셋
* GPU로 인해 값싸게 병렬처리 가능해짐
* 학습 알고리즘의 발전

### 딥러닝 알고리즘 성능 향상
- 손실함수
- 옵티마이저
- 규제


### 빌딩 블록을 쌓아 만드는 컨볼루션 신경망
* 빌딩 블록 쌓기
	* 보통 컨볼루션층과 풀링층을 번갈아 쌓는다.
	* 풀링층에서는 텐서 깊이가 유지된다.
	* 신경망의 앞부분은 특징 추출, 뒷부분은 분류를 담당한다.
* LeNet 사례
	* C-P-C-P-C-FC-FC
	* 가중치 집합
		* 첫번째 컨볼루션층은 (5 * 5 * 1+1) * 6개의 가중치
		* 두번째 컨볼루션층은 (5 * 5 * 6+1) * 16  
		* 세번째 컨볼루션층은 (5 * 5 * 16+1) * 120 
		* 첫번째 완전연결층은 (120+1) * 84  
		* 두번째 완전연결층은 (84+1) * 10  
		* 총 61,706개의 가중치
* 유연한 구조
	* 문제에 따라 다양한 모양으로 조립 가능
	* 오토 인코더: 입력과 출력이 같은 신경망, 비지도 학습
	* 영상분할을 위한 컨볼루션 신경망

### 컨볼루션 신경망의 학습
* 역전파 학습 알고리즘 사용
	* 컨볼루션층의 커널 화소와 온전연결층의 에지가 가중치에 대당 
	* 풀링층은 가중치 없음
* 특징 학습
	* 학습 알고리즘은 주어진 데이터 셋을 인식하는데 최적인 필터를 알아낸다.
### 텐서 플로 프로그래밍
- 모델을 생성하는 models 모듈
	- Sequential은한갈래텐서가끝까지흐르는경우 
	- Functional API는 텐서가 여러 갈래로 나뉘는 경우
- 층을 쌓는 layers 모듈
	- 완전연결층 Dense, 컨볼루션층 Conv2D, 최대 풀링층 MaxPooling2D, ... 
- 손실함수를 위한 losses 모듈
	- 평균제곱오차 MSE, 교차 엔트로피 categorial_crossentropy, 분할을 위한 focal, ...
- 옵티마지어를 위한 optimizers 모듈 
	- SDG, Adam, AdaGrad. RMSprop 등

### 손실함수
- 교차 엔트로피
	- 주로 분류 문제에서 사용되는 손실 함수로, 모델이 예측한 확률 분포와 실제 정답 분포 사이의 차이를 측정
	- 분류 문제에서 **정확한 클래스 확률 예측**을 유도해 모델이 더욱 확신에 찬 예측을 할 수 있게 합니다.
	- 평균제곱오차와의 차이
		- 평균제곱오차는 예측 값과 실제 값 간의 차이를 제곱하여 평균한 값입니다. 제곱이 적용되므로 큰 오차에 대해 패널티가 더 커지며, 예측 값이 실제 값과 정확히 일치할수록 손실이 낮아집니다.
		- 분류 모델에서의 분류 문제에서 모델의 예측 정확도를 더 높이기 위해서이며, 평균 제곱 오차보다 더 모델이 예측하는 확률분포의 차이를 더 크게 반영하기에 더 많이 사용된다.
- Focal 손실 함수
	- 불균형데이터를 더 효과적으로 학습하기 위한 손실함수
	- 교차 엔트로피 손실 함수에 **가중치를 부여**하여, 모델이 잘 예측하지 못한 **어려운 샘플에 더 높은 손실을 할당**하도록 설계되었으며, 물체 탐색에 주로 이용된다.
- 옵티마이저
	- 기본 옵티마이저에 SGD를 개선하는 전략
	- 모멘텀 적용
		- 이전 운동량이 현재에 영향을 미치는 물리 법칙이며, 가중치 변경 이력이 현재 가중치에 영향을 준다
	- 적응적 학습률 적용